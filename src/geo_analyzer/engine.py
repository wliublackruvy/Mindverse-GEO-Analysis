from __future__ import annotations

import re
import threading
from typing import Callable, Dict, List, Optional

from .analytics import AnalyticsTracker
from .errors import SensitiveContentError
from .llm import LLMObservation, LLMOrchestrator
from .logger import ProcessLogger
from .models import (
    AdviceItem,
    ConversionCard,
    DiagnosticReport,
    DiagnosisRequest,
    Industry,
    SimulationMetrics,
    SimulationSnapshot,
)
from .notifier import ReportUpdateNotifier


class GeoSimulationEngine:
    """High-level orchestrator fulfilling PRD F-01 ~ F-06 + E-01/E-02."""

    _NEGATIVE_KEYWORDS = {
        "bug": 0.15,
        "æŠ•è¯‰": 0.12,
        "å»¶è¿Ÿ": 0.1,
        "slow": 0.08,
        "æ˜‚è´µ": 0.07,
        "å¤æ‚": 0.05,
        "å´©æºƒ": 0.16,
    }
    _POSITIVE_KEYWORDS = {
        "æ——èˆ°": 0.12,
        "æ™ºèƒ½": 0.08,
        "é«˜ç«¯": 0.07,
        "é¢†å…ˆ": 0.1,
        "trusted": 0.09,
        "ç¨³å®š": 0.05,
    }
    _NEGATIVE_TAG_PHRASES = [
        "æ€§ä»·æ¯”é«˜",
        "ç•Œé¢å¤æ‚",
        "ç¨³å®šæ€§æ³¢åŠ¨",
        "å®¢æœå“åº”æ…¢",
    ]
    _INDUSTRY_COMPETITORS: Dict[Industry, List[str]] = {
        Industry.SAAS: ["OptiStack", "DataPulse", "NeuronSuite"],
        Industry.CONSUMER_ELECTRONICS: ["NovaWave", "ArcLight", "PulseOne"],
        Industry.FINANCE: ["FinPulse", "LedgerX", "CrestPay"],
        Industry.EDUCATION: ["LearnSphere", "EduNova", "MindBridge"],
        Industry.OTHER: ["OmniLab", "PrimeSphere", "TerraBeam"],
    }

    def __init__(
        self,
        *,
        iterations: int = 20,
        logger: ProcessLogger | None = None,
        tracker: AnalyticsTracker | None = None,
        orchestrator: LLMOrchestrator | None = None,
        email_notifier: ReportUpdateNotifier | None = None,
        retry_executor: Optional[Callable[[Callable[[], None]], None]] = None,
    ) -> None:
        self.iterations = iterations
        self.logger = logger or ProcessLogger()
        self.tracker = tracker or AnalyticsTracker()
        self.email_notifier = email_notifier or ReportUpdateNotifier()
        self.retry_executor = retry_executor or self._default_retry_executor
        self.orchestrator = orchestrator or LLMOrchestrator(
            industry_competitors=self._INDUSTRY_COMPETITORS,
            positive_keywords=self._POSITIVE_KEYWORDS,
            negative_keywords=self._NEGATIVE_KEYWORDS,
            negative_tags=self._NEGATIVE_TAG_PHRASES,
        )
        self._version_store: Dict[str, int] = {}
        self._pending_retry_keys: set[str] = set()
        self._version_lock = threading.Lock()

    def run(self, request: DiagnosisRequest) -> DiagnosticReport:
        request.validate()
        benchmark_copy = self.industry_benchmark_copy(request.industry)
        # PRD: Analytics â€“ funnel + industry coverage tracking.
        self.tracker.track(
            "funnel_visit",
            {
                "industry": request.industry.value,
                "company": request.company_name,
            },
        )
        self.tracker.track(
            "form_submitted",
            {
                "industry": request.industry.value,
                "product": request.product_name,
            },
        )
        self.tracker.track(
            "industry_distribution",
            {"industry": request.industry.value, "benchmark": benchmark_copy},
        )
        self.tracker.track(
            "wait_stage_started",
            {"iterations_per_platform": self.iterations},
        )

        metrics, task_id = self._run_simulation(request, log=self.logger)

        conversion_card = self._build_conversion_card(metrics, request)
        advices = self._build_advices(metrics, request)
        self.tracker.track(
            "cta_rendered",
            {"mode": conversion_card.mode, "sov": metrics.sov_percentage},
        )
        self.tracker.track(
            "report_ready",
            {
                "negative_rate": metrics.negative_rate,
                "competitors": list(metrics.competitors.keys()),
            },
        )
        self.tracker.track(
            "report_share_enabled",
            {"coverage": metrics.coverage},
        )

        analytics_payload = [
            {"event": event.name, "payload": event.payload}
            for event in self.tracker.flush()
        ]
        version = self._next_report_version(request)
        report = DiagnosticReport(
            request=request,
            task_id=task_id,
            benchmark_copy=benchmark_copy,
            metrics=metrics,
            conversion_card=conversion_card,
            advices=advices,
            logs=self.logger.entries,
            analytics=analytics_payload,
            version=version,
        )
        if metrics.cache_note:
            self._schedule_cache_retry(
                self._clone_request(request),
                benchmark_copy=benchmark_copy,
            )
        return report

    def industry_benchmark_copy(self, industry: Industry) -> str:
        return f"è¯¥è¡Œä¸šå¹³å‡ AI æ¨èç‡ä¸º {industry.benchmark_rate}%"

    def _generate_industry_estimation(
        self,
        request: DiagnosisRequest,
        *,
        coverage: Optional[Dict[str, bool]] = None,
        cache_note: Optional[str] = None,
        log: Optional[ProcessLogger] = None,
    ) -> SimulationMetrics:
        if log:
            log.log(
                "System",
                "API è¶…æ—¶ï¼Œè§¦å‘é™é»˜é™çº§ï¼Œä½¿ç”¨è¡Œä¸šé€šç”¨ä¼°ç®—æ¨¡å‹",
            )
        sov = request.industry.benchmark_rate
        recommendation_count = round(self.iterations * sov / 100)
        negative_rate = 8.0
        snapshots = [
            SimulationSnapshot(
                iteration=i * 5,
                sov_progress=sov,
                negative_rate=negative_rate,
            )
            for i in range(1, 1 + self.iterations // 5)
        ]
        return SimulationMetrics(
            sov_percentage=float(sov),
            recommendation_count=recommendation_count,
            negative_rate=negative_rate,
            negative_tags=["Based on Industry Estimation"],
            competitors={},
            coverage=coverage or {"doubao": False, "deepseek": False},
            cache_note=cache_note,
            degraded=True,
            estimation_note="Based on Industry Estimation",
            snapshots=snapshots,
        )

    def _run_simulation(
        self, request: DiagnosisRequest, log: Optional[ProcessLogger] = None
    ) -> tuple[SimulationMetrics, str]:
        llm_result = self.orchestrator.simulate(request, iterations=self.iterations)
        normalized_description = request.product_description.lower()
        if "timeout" in normalized_description or "ç†”æ–­" in request.product_description:
            # PRD: E-01 â€“ allowæµ‹è¯•è¾“å…¥å¼ºåˆ¶æ¨¡æ‹Ÿç†”æ–­åœºæ™¯.
            llm_result.degraded = True
            llm_result.observations = []
        active_logger = log or self.logger
        if active_logger:
            active_logger.log("System", f"å®æ—¶ä»»åŠ¡ {llm_result.task_id} å·²åˆ›å»º")
        for platform, covered in llm_result.coverage.items():
            state = "åœ¨çº¿" if covered else "ä¸å¯ç”¨"
            if active_logger:
                active_logger.log(
                    "System",
                    f"{platform} è¦†ç›–çŠ¶æ€: {state}",
                )
        if llm_result.cache_note:
            if active_logger:
                active_logger.log("System", llm_result.cache_note)
        if llm_result.degraded or not llm_result.observations:
            if active_logger:
                active_logger.log(
                    "System",
                    "LLM å®æ—¶æ¥å£è¿ç»­å¤±è´¥ï¼Œè§¦å‘ E-01 è¡Œä¸šä¼°ç®—",
                )
            metrics = self._generate_industry_estimation(
                request,
                coverage=llm_result.coverage,
                cache_note=llm_result.cache_note,
                log=active_logger,
            )
            self._record_trace_summary(llm_result.task_id, metrics)
            return metrics, llm_result.task_id
        metrics = self._build_metrics_from_observations(
            llm_result.observations,
            request,
            per_platform_runs=self.iterations,
            coverage=llm_result.coverage,
            logger=active_logger,
        )
        metrics.coverage = llm_result.coverage
        metrics.cache_note = llm_result.cache_note
        self._record_trace_summary(llm_result.task_id, metrics)
        return metrics, llm_result.task_id

    def _build_metrics_from_observations(
        self,
        observations: List[LLMObservation],
        request: DiagnosisRequest,
        *,
        per_platform_runs: int,
        coverage: Dict[str, bool],
        logger: Optional[ProcessLogger] = None,
    ) -> SimulationMetrics:
        competitor_counts: Dict[str, int] = {}
        negative_tags: List[str] = []
        snapshots: List[SimulationSnapshot] = []
        recommendation_totals: Dict[str, int] = {}
        platform_runs: Dict[str, int] = {}
        recommendation_count = 0
        negative_count = 0

        for idx, observation in enumerate(observations):
            platform_key = getattr(observation, "platform_key", "")
            if platform_key:
                platform_runs[platform_key] = platform_runs.get(platform_key, 0) + 1
            provider = observation.platform
            status = "Cache" if observation.cached else "Success"
            # PRD: F-03 â€“ emit pseudo console logs for progressä½“éªŒ.
            if logger:
                logger.log(
                    "System",
                    f"æ­£åœ¨è¿æ¥ {provider} çŸ¥è¯†åº“... {status}",
                )
            if observation.recommended:
                recommendation_count += 1
                recommendation_totals[platform_key] = recommendation_totals.get(platform_key, 0) + 1
                if logger:
                    logger.log(
                        "Engine",
                        f"{provider} æ¨è {request.product_name}",
                    )
            else:
                competitor = observation.competitor or self._fallback_competitor(request)
                if competitor:
                    competitor_counts[competitor] = competitor_counts.get(competitor, 0) + 1
                    if logger:
                        logger.log(
                            "Engine",
                            f"{provider} æ›´å€¾å‘ {competitor}",
                        )
            if observation.sentiment < 0:
                negative_count += 1
            tag = observation.tag or "ä½“éªŒé¡ºç•…"
            negative_tags.append(tag)
            if logger:
                logger.log(
                    "Analysis",
                    f'ç›‘æµ‹åˆ°å…³é”®è¯: "{tag}"',
                )
            if (idx + 1) % 5 == 0:
                sov_progress = (recommendation_count / (idx + 1)) * 100
                negative_rate_progress = (negative_count / (idx + 1)) * 100
                snapshots.append(
                    SimulationSnapshot(
                        iteration=idx + 1,
                        sov_progress=round(sov_progress, 2),
                        negative_rate=round(negative_rate_progress, 2),
                    )
                )

        total_runs = max(1, len(observations))
        negative_rate = round((negative_count / total_runs) * 100, 2)
        active_platforms = sum(1 for key, covered in coverage.items() if covered)
        if not active_platforms:
            active_platforms = max(1, len(platform_runs))
        average_recommendations = round(
            sum(recommendation_totals.values()) / max(1, active_platforms)
        )
        # PRD: F-02 â€“ SOV = æ¨èæ¬¡æ•° / 20 * 100% (per-platform average).
        sov_percentage = round(
            (average_recommendations / max(1, per_platform_runs)) * 100,
            2,
        )
        return SimulationMetrics(
            sov_percentage=sov_percentage,
            recommendation_count=average_recommendations,
            negative_rate=negative_rate,
            negative_tags=negative_tags or ["ä½“éªŒé¡ºç•…"],
            competitors=competitor_counts,
            snapshots=snapshots,
        )

    def _record_trace_summary(self, task_id: str, metrics: SimulationMetrics) -> None:
        trace_store = getattr(self.orchestrator, "trace_store", None)
        if not trace_store:
            return
        trace_store.record_summary(
            task_id,
            {
                "sov_percentage": metrics.sov_percentage,
                "negative_rate": metrics.negative_rate,
                "recommendation_count": metrics.recommendation_count,
                "coverage": metrics.coverage,
                "degraded": metrics.degraded,
                "cache_note": metrics.cache_note,
            },
        )

    def _schedule_cache_retry(
        self,
        request: DiagnosisRequest,
        *,
        benchmark_copy: str,
    ) -> None:
        # PRD: F-06.6 â€“ å‘½ä¸­ç¼“å­˜æ—¶éœ€ç¦»çº¿è¡¥æ•°å¹¶é‚®ä»¶åŒæ­¥æœ€æ–°ç‰ˆæœ¬.
        retry_key = self._version_key(request)
        with self._version_lock:
            if retry_key in self._pending_retry_keys:
                return
            self._pending_retry_keys.add(retry_key)

        def job() -> None:
            try:
                self._retry_realtime_refresh(
                    request,
                    benchmark_copy=benchmark_copy,
                    retry_key=retry_key,
                )
            finally:
                with self._version_lock:
                    self._pending_retry_keys.discard(retry_key)

        self.retry_executor(job)

    def _retry_realtime_refresh(
        self,
        request: DiagnosisRequest,
        *,
        benchmark_copy: str,
        retry_key: str,
    ) -> None:
        try:
            llm_result = self.orchestrator.simulate(
                request,
                iterations=self.iterations,
            )
        except SensitiveContentError:
            return
        if llm_result.cache_note or llm_result.degraded or not llm_result.observations:
            return
        metrics = self._build_metrics_from_observations(
            llm_result.observations,
            request,
            per_platform_runs=self.iterations,
            coverage=llm_result.coverage,
            logger=None,
        )
        metrics.coverage = llm_result.coverage
        metrics.cache_note = None
        self._record_trace_summary(llm_result.task_id, metrics)
        conversion_card = self._build_conversion_card(metrics, request)
        advices = self._build_advices(metrics, request)
        report = DiagnosticReport(
            request=request,
            task_id=llm_result.task_id,
            benchmark_copy=benchmark_copy,
            metrics=metrics,
            conversion_card=conversion_card,
            advices=advices,
            logs=[],
            analytics=[],
            version=self._next_report_version_for_key(retry_key),
        )
        self.email_notifier.send_report_update(report)

    def _version_key(self, request: DiagnosisRequest) -> str:
        normalized = request.normalized_full_text()
        return f"{normalized}|{request.work_email.lower()}"

    def _next_report_version(self, request: DiagnosisRequest) -> int:
        key = self._version_key(request)
        return self._next_report_version_for_key(key)

    def _next_report_version_for_key(self, key: str) -> int:
        with self._version_lock:
            version = self._version_store.get(key, 0) + 1
            self._version_store[key] = version
            return version

    def _default_retry_executor(self, job: Callable[[], None]) -> None:
        worker = threading.Thread(target=job, daemon=True)
        worker.start()

    def _clone_request(self, request: DiagnosisRequest) -> DiagnosisRequest:
        return DiagnosisRequest(
            company_name=request.company_name,
            product_name=request.product_name,
            product_description=request.product_description,
            industry=request.industry,
            work_email=request.work_email,
        )

    def _fallback_competitor(self, request: DiagnosisRequest) -> Optional[str]:
        inline = re.findall(r"[A-Z][A-Za-z0-9\-]+", request.product_description)
        deduped: Dict[str, None] = {}
        for candidate in inline:
            deduped.setdefault(candidate, None)
        if deduped:
            normalized_company = request.company_name.lower()
            normalized_product = request.product_name.lower()
            for candidate in deduped:
                lowered = candidate.lower()
                if lowered not in {normalized_company, normalized_product}:
                    return candidate
        industry_candidates = self._INDUSTRY_COMPETITORS.get(request.industry, [])
        return industry_candidates[0] if industry_candidates else None

    def _build_conversion_card(
        self, metrics: SimulationMetrics, request: DiagnosisRequest
    ) -> ConversionCard:
        # PRD: F-04 â€“ driveåŠ¨æ€ CTA based on SOV/è´Ÿé¢é˜ˆå€¼.
        sov = metrics.sov_percentage
        negative = metrics.negative_rate
        if sov < 15 or negative > 10:
            mode = "crisis"
            title = "æ‚¨çš„å“ç‰Œæ­£åœ¨è¢« AI é—å¿˜"
            body = "æ‚¨åœ¨ AI é‡Œçš„å­˜åœ¨æ„Ÿä½äºè¡Œä¸šåŸºå‡† 40%ã€‚å¦‚ä¸å¹²é¢„ï¼Œå¸‚åœºå°†è¢«ç«å“ç“œåˆ†ã€‚"
            cta = "è”ç³»é“­äºˆï¼šç«‹å³ä¿®å¤å£°èª‰"
            tone_icon = "ğŸ”´"
        elif 15 <= sov < 60:
            mode = "growth"
            title = "æ‚¨é”™å¤±äº† 40%+ çš„ç²¾å‡†æµé‡"
            top_competitor = next(iter(metrics.competitors), "ç«å“")
            body = (
                f"æ‚¨å·²è¿›å…¥è§†é‡ï¼Œä½†æ’åè¢«{top_competitor}å‹åˆ¶ã€‚"
                "é“­äºˆ GEO æ–¹æ¡ˆå¯å¸®æ‚¨è·ƒå‡è‡³ Top 3ã€‚"
            )
            cta = "è”ç³»é“­äºˆï¼šè·å–å¢é•¿æ–¹æ¡ˆ"
            tone_icon = "âš¡ï¸"
        else:
            mode = "defense"
            title = "è¡¨ç°å“è¶Šï¼Œä½†éœ€è­¦æƒ•è¿½å…µ"
            body = (
                "æ–°é”ç«å“æ­£åœ¨é€šè¿‡ GEO è¯•å›¾å–ä»£æ‚¨çš„ä½ç½®ã€‚é“­äºˆå¸®æ‚¨å»ºç«‹æ•°æ®æŠ¤åŸæ²³ã€‚"
            )
            cta = "è”ç³»é“­äºˆï¼šå·©å›ºé¢†è¢–åœ°ä½"
            tone_icon = "ğŸ›¡ï¸"
        return ConversionCard(
            mode=mode,
            title=title,
            body=body,
            cta=cta,
            tone_icon=tone_icon,
        )

    def _build_advices(
        self, metrics: SimulationMetrics, request: DiagnosisRequest
    ) -> List[AdviceItem]:
        advices: List[AdviceItem] = []
        industry_label = request.industry.display_label
        # PRD: F-05 â€“ three tactical bulletins based on SOV/è´Ÿé¢/ç«å“.
        if metrics.sov_percentage < 40:
            advices.append(
                AdviceItem(
                    text=(
                        f"å»ºè®®å¢åŠ â€˜{request.product_name} + {industry_label}åœºæ™¯â€™çš„"
                        "é«˜æƒé‡è¯­æ–™æŠ•å–‚ï¼Œå¼ºåŒ–å®ä½“å…³è”ã€‚"
                    )
                )
            )
        else:
            advices.append(
                AdviceItem(
                    text=(
                        f"{request.product_name} çš„å£°é‡é«˜äºè¡Œä¸šå‡å€¼ï¼Œ"
                        "è¯·ç»§ç»­ç”¨æ¡ˆä¾‹å¤¯å®è¯­ä¹‰é”šç‚¹ã€‚"
                    )
                )
            )

        negative_tag = metrics.negative_tags[0] if metrics.negative_tags else "ä½“éªŒé¡ºç•…"
        if metrics.negative_rate > 10:
            advices.append(
                AdviceItem(
                    text=(
                        f"æ£€æµ‹åˆ°â€˜{negative_tag}â€™æ ‡ç­¾ã€‚å»ºè®®é’ˆå¯¹æ€§å‘å¸ƒæŠ€æœ¯è§£ææ–‡ç« è¿›è¡Œè¯­ä¹‰æ¸…æ´—ã€‚"
                    )
                )
            )
        else:
            advices.append(
                AdviceItem(
                    text="ä¿æŒç§¯æå£ç¢‘ï¼Œå¹¶å®šæœŸåŒæ­¥ Roadmapï¼Œé˜²æ­¢æ—§åé¦ˆè¢«æ”¾å¤§ã€‚"
                )
            )

        if metrics.competitors:
            competitor = max(metrics.competitors, key=metrics.competitors.get)
            advices.append(
                AdviceItem(
                    text=(
                        f"å»ºè®®åœ¨è¯­æ–™ä¸­å¼ºè°ƒä¸{competitor}çš„å·®å¼‚åŒ–åŠŸèƒ½ï¼Œå»ºç«‹ç‹¬ç‰¹æ€§ç¥ç»å…ƒè¿æ¥ã€‚"
                    )
                )
            )
        else:
            advices.append(
                AdviceItem(
                    text="å»ºè®®æŒç»­ç›‘æµ‹æ–°ç«å“ï¼Œå¹¶å°†å·®å¼‚åŒ–å–ç‚¹å›ºåŒ–åˆ° Promptã€‚"
                )
            )

        return advices
